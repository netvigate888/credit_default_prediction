{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "# Creating the Train and Test Sets\n",
    "Here we create the final train and test sets ready for predictive modelling.\n",
    "\n",
    "1. [Setting up the Environment](#envir)\n",
    "1. [Loading Data Post Correlation Analysis](#load)\n",
    "1. [Split into Train and Test Sets](#split)\n",
    "1. [Imputing Missing Values from Training Set](#impute)\n",
    "1. [Standard Scaling](#standard)\n",
    "1. [Dropping SK_ID_CURR and TARGET Attributes](#drop)\n",
    "1. [Saving the Train and Test Sets](#save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='envir'></a>\n",
    "# 1. Setting up the Environment\n",
    "\n",
    "[Return](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import nan as NaN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score, confusion_matrix\n",
    "#from sklearn.model_selection import KFold\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.max_columns = 200\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load'></a>\n",
    "# 2. Loading Data Post Correlation Analysis\n",
    "\n",
    "[Return](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe (307511, 261)\n"
     ]
    }
   ],
   "source": [
    "# Read in the data post correlation analysis\n",
    "path = 'C:/Users/X/Documents/A_Documents/Cap_Data/CSV'\n",
    "app = pd.read_csv(path + '/app_less_corr/app_less_corr.csv')\n",
    "print('Shape of dataframe ' + str(app.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='split'></a>\n",
    "# 3. Split into Train and Test Sets\n",
    "\n",
    "[Return](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the data in train and test sets on a stratified basis around the TARGET attribute\n",
    "# random_state = 777 is used to ensure we consistently get the same train and test sets\n",
    "train_set, test_set = train_test_split(app, test_size=0.3, random_state=777, stratify=app['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "0    0.919273\n",
      "1    0.080727\n",
      "Name: TARGET, dtype: float64\n",
      "\n",
      "Testing Set:\n",
      "0    0.919266\n",
      "1    0.080734\n",
      "Name: TARGET, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# A quick check of the stratification\n",
    "print('Training Set:')\n",
    "print(train_set['TARGET'].value_counts()/len(train_set))\n",
    "print('\\nTesting Set:')\n",
    "print(test_set['TARGET'].value_counts()/len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just checking that we do actually get consistent train/test splits\n",
    "#tr = pd.read_csv(path + '/train_test_IDs/train_IDs.csv')\n",
    "#te = pd.read_csv(path + '/train_test_IDs/test_IDs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be zero\n",
    "#sum(~(np.sort(tr['SK_ID_CURR'].values) == np.sort(train_set['SK_ID_CURR'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be zero\n",
    "#sum(~(np.sort(te['SK_ID_CURR'].values) == np.sort(test_set['SK_ID_CURR'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the TARGET attribute to make label sets\n",
    "train_labels = train_set[['TARGET']].copy()\n",
    "test_labels = test_set[['TARGET']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='impute'></a>\n",
    "# 4. Imputing Missing Values from Training Set\n",
    "\n",
    "[Return](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the imputer on the training set ONLY\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "imputer.fit(train_set)\n",
    "\n",
    "# Running the imputer across trian_set and test_set\n",
    "train_set = imputer.transform(train_set)\n",
    "test_set = imputer.transform(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standard'></a>\n",
    "# 5. Standard Scaling\n",
    "\n",
    "[Return](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Training the scaler on the training set ONLY\n",
    "scaler.fit(train_set)\n",
    "\n",
    "# Running the scaler across the the training and test sets\n",
    "train_set = scaler.transform(train_set)\n",
    "test_set = scaler.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: (215257, 261)\n",
      "test_set: (92254, 261)\n"
     ]
    }
   ],
   "source": [
    "print('train_set: {}'.format(train_set.shape))\n",
    "print('test_set: {}'.format(test_set.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='drop'></a>\n",
    "# 6. Dropping SK_ID_CURR and TARGET Attributes\n",
    "\n",
    "[Return](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting back to dataframes. Not really required but keeps the data tidy.\n",
    "train_set = pd.DataFrame(train_set, columns=app.columns)\n",
    "test_set = pd.DataFrame(test_set, columns=app.columns)\n",
    "\n",
    "# Dropping SK_ID_CURR (no longer required as it's just an ID) and the TARGET attribute from the train and test sets\n",
    "train_set = train_set.drop(['SK_ID_CURR','TARGET'], axis=1)\n",
    "test_set = test_set.drop(['SK_ID_CURR','TARGET'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='save'></a>\n",
    "# 7. Saving the Train and Test Sets\n",
    "\n",
    "[Return](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(path+'/train_test_sets/train_set_std.csv', index=False)\n",
    "train_labels.to_csv(path+'/train_test_sets/train_labels.csv', index=False)\n",
    "test_set.to_csv(path+'/train_test_sets/test_set_std.csv', index=False)\n",
    "test_labels.to_csv(path+'/train_test_sets/test_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
